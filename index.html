<!DOCTYPE html>
<html lang="tr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Dual DGX Spark â€” Coding Agent MÃ¼hendislik PlanÄ±</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Outfit:wght@400;500;600;700;800&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
:root{
  --bg:#06060a;--s1:#0e0e14;--s2:#14141c;--card:#161620;
  --brd:#1e1e2a;--brd2:#2a2a3a;
  --t0:#f2f2f8;--t1:#c4c4d4;--t2:#8888a0;--t3:#555568;
  --grn:#00e68a;--blu:#4d94ff;--red:#ff5c6e;--amb:#ffb84d;--pur:#b08cff;--cyn:#00d4ff;--pink:#ff6eb4;
  --grn-g:rgba(0,230,138,.06);--blu-g:rgba(77,148,255,.06);--red-g:rgba(255,92,110,.06);--amb-g:rgba(255,184,77,.06);--pur-g:rgba(176,140,255,.06);
}
body{background:var(--bg);color:var(--t0);font-family:'Outfit',system-ui,sans-serif;line-height:1.6;-webkit-font-smoothing:antialiased}
code,.mono{font-family:'JetBrains Mono',monospace;font-size:12px}
::-webkit-scrollbar{width:5px}::-webkit-scrollbar-thumb{background:var(--brd2);border-radius:4px}

.wrap{max-width:960px;margin:0 auto;padding:28px 20px 80px}

/* â•â•â• HERO â•â•â• */
.hero{text-align:center;padding:40px 0 32px;position:relative}
.hero::before{content:'';position:absolute;top:-20px;left:50%;transform:translateX(-50%);width:600px;height:350px;background:radial-gradient(ellipse,rgba(0,230,138,.06) 0%,rgba(77,148,255,.04) 40%,transparent 70%);pointer-events:none}
.chip{display:inline-flex;align-items:center;gap:7px;background:linear-gradient(135deg,rgba(0,230,138,.1),rgba(77,148,255,.08));border:1px solid rgba(0,230,138,.2);border-radius:999px;padding:5px 16px;font-size:11px;color:var(--grn);font-weight:600;letter-spacing:.5px;text-transform:uppercase;margin-bottom:14px}
.chip .pulse{width:7px;height:7px;border-radius:50%;background:var(--grn);animation:pulse 2s infinite}
@keyframes pulse{0%,100%{opacity:1}50%{opacity:.25}}
.hero h1{font-size:clamp(26px,5vw,38px);font-weight:800;letter-spacing:-.5px;background:linear-gradient(135deg,#fff 40%,var(--grn) 90%);-webkit-background-clip:text;-webkit-text-fill-color:transparent;margin-bottom:8px}
.hero p{color:var(--t2);font-size:14px;max-width:640px;margin:0 auto;line-height:1.6}
.hero-tags{display:flex;justify-content:center;gap:16px;margin-top:16px;flex-wrap:wrap}
.hero-tag{font-size:12px;color:var(--t3);display:flex;align-items:center;gap:5px}

/* â•â•â• SPECS â•â•â• */
.specs{display:grid;grid-template-columns:repeat(6,1fr);gap:8px;margin-bottom:28px}
.spec{background:var(--card);border:1px solid var(--brd);border-radius:12px;padding:14px 10px;text-align:center;transition:all .2s}
.spec:hover{border-color:var(--brd2);transform:translateY(-2px)}
.spec .emoji{font-size:20px;margin-bottom:4px}
.spec .val{font-size:17px;font-weight:700;font-family:'JetBrains Mono',monospace}
.spec .lbl{font-size:9px;color:var(--t3);text-transform:uppercase;letter-spacing:.5px;margin-top:2px}

/* â•â•â• NAV â•â•â• */
.nav{display:flex;gap:3px;background:var(--s1);border:1px solid var(--brd);border-radius:12px;padding:4px;margin-bottom:24px;position:sticky;top:8px;z-index:100;backdrop-filter:blur(16px);-webkit-backdrop-filter:blur(16px)}
.nav button{flex:1;padding:10px 4px;border:none;border-radius:8px;background:transparent;color:var(--t2);font-family:'Outfit',sans-serif;font-size:12px;font-weight:600;cursor:pointer;transition:all .2s;white-space:nowrap}
.nav button.on{background:var(--card);color:var(--t0);box-shadow:0 2px 10px rgba(0,0,0,.4)}
.nav button:hover:not(.on){color:var(--t1)}

/* â•â•â• SECTIONS â•â•â• */
.sec{display:none}.sec.vis{display:block;animation:fadeIn .25s ease}
@keyframes fadeIn{from{opacity:0;transform:translateY(6px)}to{opacity:1;transform:translateY(0)}}

/* â•â•â• PHASE HEADER â•â•â• */
.ph{margin-bottom:28px}
.ph-hdr{display:flex;align-items:center;gap:14px;margin-bottom:16px;padding-bottom:14px;border-bottom:1px solid var(--brd)}
.ph-icon{width:44px;height:44px;border-radius:12px;display:flex;align-items:center;justify-content:center;font-size:22px;flex-shrink:0}
.ph-info h2{font-size:18px;font-weight:700;letter-spacing:-.2px}
.ph-info p{font-size:13px;color:var(--t2);margin-top:2px;line-height:1.5}

/* â•â•â• SPARK LAYOUT â•â•â• */
.spark-grid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin-bottom:16px}
.spark-card{background:var(--card);border-radius:12px;padding:16px;border:1px solid}
.spark-card h4{font-size:13px;font-weight:700;margin-bottom:10px;display:flex;align-items:center;gap:8px}
.spark-card .badge{font-size:10px;color:var(--t3);margin-left:auto;background:var(--s1);padding:2px 8px;border-radius:6px}
.si{font-size:13px;color:var(--t1);padding:3px 0;display:flex;align-items:center;gap:7px}
.si .arr{font-size:11px;font-weight:700;width:14px;text-align:center;flex-shrink:0}

/* â•â•â• BLOCK â•â•â• */
.blk{background:var(--card);border:1px solid var(--brd);border-radius:12px;margin-bottom:10px;overflow:hidden;transition:border-color .2s}
.blk:hover{border-color:var(--brd2)}
.blk-h{padding:12px 16px;display:flex;align-items:center;justify-content:space-between;cursor:pointer;user-select:none;font-size:13px;font-weight:600;transition:background .15s}
.blk-h:hover{background:rgba(255,255,255,.015)}
.blk-h .left{display:flex;align-items:center;gap:8px}
.blk-h .tag{font-size:9px;padding:2px 8px;border-radius:5px;font-weight:600;letter-spacing:.4px;text-transform:uppercase}
.blk-h .chevron{color:var(--t3);font-size:14px;transition:transform .2s}
.blk-b{padding:14px 16px;border-top:1px solid var(--brd)}
.blk-b.hide{display:none}

/* â•â•â• ITEMS â•â•â• */
.it{display:flex;align-items:flex-start;gap:10px;padding:4px 0;font-size:13px;color:var(--t1);line-height:1.55}
.it .bullet{flex-shrink:0;margin-top:3px;font-size:11px}
.it code{background:rgba(0,0,0,.35);padding:1px 6px;border-radius:3px;font-size:11px;color:var(--grn)}

/* â•â•â• CMD â•â•â• */
.cmd{background:var(--bg);border:1px solid var(--brd);border-radius:8px;padding:12px 14px;margin:10px 0;overflow-x:auto}
.cmd code{color:var(--grn);font-size:11px;line-height:1.7;white-space:pre}
.cmd .c{color:var(--t3)}

/* â•â•â• TABLE â•â•â• */
.tbl{width:100%;border-collapse:collapse;font-size:12px;margin:10px 0}
.tbl th{text-align:left;padding:8px 10px;background:var(--bg);color:var(--t2);font-weight:600;font-size:10px;text-transform:uppercase;letter-spacing:.4px;border-bottom:1px solid var(--brd)}
.tbl td{padding:8px 10px;border-bottom:1px solid rgba(30,30,42,.6);color:var(--t1);font-size:12px}
.tbl tr:hover td{background:rgba(255,255,255,.012)}

/* â•â•â• GOV GRID â•â•â• */
.gov{display:grid;grid-template-columns:1fr 1fr;gap:8px;margin:8px 0}
.gov-cell{background:var(--bg);border:1px solid var(--brd);border-radius:8px;padding:10px 12px}
.gov-cell .gl{font-size:9px;color:var(--t3);text-transform:uppercase;letter-spacing:.4px;margin-bottom:3px}
.gov-cell .gv{font-size:12px;font-weight:600;color:var(--t1);font-family:'JetBrains Mono',monospace}

/* â•â•â• SUCCESS â•â•â• */
.success{background:var(--grn-g);border:1px solid rgba(0,230,138,.15);border-radius:12px;padding:16px;margin-top:14px}
.success h4{font-size:12px;color:var(--grn);font-weight:700;text-transform:uppercase;letter-spacing:.5px;margin-bottom:10px}
.success .it .bullet{color:var(--grn)}

/* â•â•â• WARN / DANGER â•â•â• */
.warn{background:var(--amb-g);border:1px solid rgba(255,184,77,.15);border-radius:10px;padding:14px 16px;margin:10px 0}
.warn h4{font-size:11px;color:var(--amb);font-weight:700;text-transform:uppercase;letter-spacing:.4px;margin-bottom:8px}
.warn p,.warn .it{font-size:13px;color:var(--t1)}
.danger{background:var(--red-g);border:1px solid rgba(255,92,110,.15);border-radius:10px;padding:14px 16px;margin:10px 0}
.danger h4{font-size:11px;color:var(--red);font-weight:700;text-transform:uppercase;letter-spacing:.4px;margin-bottom:8px}

/* â•â•â• RAM BAR â•â•â• */
.ram-row{display:flex;align-items:center;gap:8px;margin-bottom:5px}
.ram-lbl{width:160px;font-size:12px;color:var(--t1);flex-shrink:0}
.ram-bg{flex:1;height:20px;background:var(--bg);border-radius:5px;overflow:hidden;border:1px solid var(--brd)}
.ram-fill{height:100%;border-radius:4px;display:flex;align-items:center;justify-content:flex-end;padding-right:7px;min-width:32px}
.ram-fill span{font-size:9px;font-family:'JetBrains Mono',monospace;color:#fff;font-weight:600}

/* â•â•â• RESPONSIVE â•â•â• */
@media(max-width:768px){
  .specs{grid-template-columns:repeat(3,1fr)}
  .spark-grid{grid-template-columns:1fr}
  .gov{grid-template-columns:1fr}
  .nav button{font-size:11px;padding:8px 3px}
}
@media(max-width:480px){.specs{grid-template-columns:repeat(2,1fr)}}
</style>
</head>
<body>
<div class="wrap" id="app"></div>
<script>
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• DATA â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const phases = [
// â•â•â•â•â•â• FAZ 0 â•â•â•â•â•â•
{
  id:0, emoji:"ğŸ”§", title:"DonanÄ±m & Runtime DoÄŸrulama", color:"#00d4ff",
  obj:"Her iki DGX Spark'Ä± baÄŸÄ±msÄ±z olarak doÄŸrula. CUDA, driver, Docker ve vLLM tek-dÃ¼ÄŸÃ¼m inference testlerini tamamla. DaÄŸÄ±tÄ±k mod yok.",
  s1:["Driver & CUDA doÄŸrulama","Docker GPU passthrough testi","vLLM NGC container smoke test"],
  s2:["AynÄ± doÄŸrulama baÄŸÄ±msÄ±z olarak tekrarlanÄ±r","Spark #1 ile birebir aynÄ± kurulum"],
  blocks:[
    {name:"Driver & CUDA DoÄŸrulama",emoji:"ğŸ”",tag:"ALTYAPI",tc:"#00d4ff",items:[
      {b:"ğŸ–¥ï¸",t:"DGX OS versiyonu: <code>cat /etc/os-release</code> â†’ Ubuntu 24.04"},
      {b:"âš™ï¸",t:"CUDA toolkit: <code>nvcc --version</code> â†’ CUDA 13.0+ gerekli"},
      {b:"ğŸ®",t:"GPU gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼: <code>nvidia-smi</code> â†’ GB10 Blackwell, sm_121"},
      {b:"âš ï¸",t:"<code>nvidia-smi</code> unified memory'de <code>Memory: N/A</code> gÃ¶sterir â€” bu normal"},
      {b:"ğŸ—ï¸",t:"Mimari: <code>uname -m</code> â†’ <code>aarch64</code> (ARM64)"},
      {b:"ğŸ’¾",t:"KullanÄ±labilir bellek: <code>free -h</code> â†’ ~119 GB (GPU+CPU ortak havuz)"},
    ]},
    {name:"Docker & NVIDIA Container Toolkit",emoji:"ğŸ³",tag:"ALTYAPI",tc:"#00d4ff",items:[
      {b:"ğŸ³",t:"Docker doÄŸrula: <code>docker --version</code> â†’ Docker 24+ gerekli"},
      {b:"ğŸ”Œ",t:"NVIDIA toolkit: <code>nvidia-ctk --version</code>"},
      {b:"âœ…",t:"GPU passthrough: <code>docker run --rm --gpus all nvcr.io/nvidia/cuda:13.1.0-base-ubuntu24.04 nvidia-smi</code>"},
      {b:"ğŸ“¦",t:"NGC vLLM container Ã§ek: <code>docker pull nvcr.io/nvidia/vllm:26.01-py3</code>"},
      {b:"ğŸ”‘",t:"NGC auth gerekiyorsa: ngc.nvidia.com'a kayÄ±t â†’ <code>docker login nvcr.io</code>"},
    ]},
    {name:"vLLM Smoke Test (Tek DÃ¼ÄŸÃ¼m)",emoji:"ğŸ§ª",tag:"DOÄRULAMA",tc:"#00e68a",items:[
      {b:"ğŸš€",t:"Minimal model ile NGC container iÃ§inde test:"},
    ],cmd:`<span class="c"># vLLM container'Ä± GPU eriÅŸimiyle baÅŸlat</span>
docker run --rm -it --gpus all \\
  --shm-size=16g \\
  -v ~/.cache/huggingface:/root/.cache/huggingface \\
  -p 8000:8000 \\
  nvcr.io/nvidia/vllm:26.01-py3 \\
  vllm serve facebook/opt-125m \\
    --enforce-eager \\
    --host 0.0.0.0 --port 8000

<span class="c"># BaÅŸka bir terminal'den inference testi</span>
curl http://localhost:8000/v1/completions \\
  -H "Content-Type: application/json" \\
  -d '{"model":"facebook/opt-125m","prompt":"Merhaba","max_tokens":16}'`,
    extra:[
      {b:"âœ…",t:"YanÄ±tÄ±n geÃ§erli JSON ile token dÃ¶ndÃ¼ÄŸÃ¼nÃ¼ doÄŸrula"},
      {b:"âš ï¸",t:"<code>--enforce-eager</code> DGX Spark'ta zorunlu â€” Triton sm_121 desteÄŸi henÃ¼z olgun deÄŸil"},
      {b:"ğŸ”„",t:"AynÄ± testi Spark #2'de baÄŸÄ±msÄ±z olarak tekrarla"},
    ]},
    {name:"AÄŸ BaÄŸlantÄ±sÄ± Testi (Spark'lar ArasÄ±)",emoji:"ğŸ”—",tag:"AÄ",tc:"#4d94ff",items:[
      {b:"ğŸ”Œ",t:"QSFP56 200GbE DAC kablo ile baÄŸla (0.5m Ã¶nerilir, ~$30)"},
      {b:"ğŸ”",t:"InfiniBand arayÃ¼zÃ¼: <code>ibdev2netdev</code> veya <code>ip link show</code>"},
      {b:"ğŸ“¡",t:"Statik IP ata (doÄŸrudan baÄŸlantÄ± arayÃ¼zÃ¼ne)"},
      {b:"ğŸ“¶",t:"BaÄŸlantÄ± testi: <code>ping -c 5 &lt;spark2-ip&gt;</code>"},
      {b:"âš¡",t:"Bant geniÅŸliÄŸi: <code>iperf3</code> â†’ >25 GB/s beklenir"},
      {b:"ğŸš«",t:"Bu fazda tensor parallel veya Ray cluster kurma â€” sadece aÄŸ doÄŸrulama"},
    ]},
    {name:"SaÄŸlÄ±k Raporu Kontrol Listesi",emoji:"ğŸ“‹",tag:"KONTROL",tc:"#ffb84d",items:[
      {b:"â˜",t:"Spark #1: nvidia-smi GB10 gÃ¶steriyor"},
      {b:"â˜",t:"Spark #1: CUDA 13.x doÄŸrulandÄ±"},
      {b:"â˜",t:"Spark #1: Docker GPU passthrough Ã§alÄ±ÅŸÄ±yor"},
      {b:"â˜",t:"Spark #1: vLLM opt-125m sunuyor ve token dÃ¶nÃ¼yor"},
      {b:"â˜",t:"Spark #2: YukarÄ±dakilerin tamamÄ± tekrarlandÄ± ve doÄŸrulandÄ±"},
      {b:"â˜",t:"AÄŸ: iperf3 Spark'lar arasÄ± >20 GB/s gÃ¶steriyor"},
      {b:"â˜",t:"Her iki Spark SSH ile eriÅŸilebilir (Tailscale veya direkt)"},
    ]},
  ],
  gov:{
    "EÅŸzamanlÄ± Ä°stek":"Yok (smoke test)",
    "Batch Boyutu":"Yok",
    "Token Limiti":"Yok",
    "Yeniden Deneme":"Yok",
    "Kill Switch":"Manuel: docker stop / Ctrl+C",
    "GPU SÃ¼re Takibi":"Yok",
    "PII Maskeleme":"Yok",
  },
  success:[
    "Her iki Spark baÄŸÄ±msÄ±z olarak vLLM inference sunuyor",
    "AÄŸ baÄŸlantÄ±sÄ± >20 GB/s bant geniÅŸliÄŸinde doÄŸrulandÄ±",
    "SaÄŸlÄ±k raporu tÃ¼m maddeleri iÅŸaretlendi",
    "DaÄŸÄ±tÄ±k mod yapÄ±landÄ±rÄ±lmadÄ± veya denenmedi",
  ]
},
// â•â•â•â•â•â• FAZ 1 â•â•â•â•â•â•
{
  id:1, emoji:"ğŸ¤–", title:"Tek DÃ¼ÄŸÃ¼m Coding Agent", color:"#00e68a",
  obj:"Spark #1 Ã¼zerinde stabil coding agent. ÃœÃ§ model vLLM ile Ã§alÄ±ÅŸÄ±r, OpenClaw orkestrasyon saÄŸlar. Agent kod Ã¼retir, lint yapar, hatalarÄ± dÃ¼zeltir ve derlenen kod dÃ¶ndÃ¼rÃ¼r.",
  s1:["vLLM inference (3 model)","OpenClaw agent runtime","Coding agent mantÄ±ÄŸÄ±","Sandbox ortamÄ±"],
  s2:["BoÅŸta veya geliÅŸtirme/test aynasÄ±"],
  blocks:[
    {name:"vLLM Model Deployment",emoji:"ğŸ§ ",tag:"MODELLER",tc:"#00e68a",items:[
      {b:"ğŸ“Š",t:"Spark #1 Ã¼zerinde Ã¼Ã§ model â€” her biri ayrÄ± port'ta ayrÄ± container olarak"},
      {b:"ğŸ·ï¸",t:"Tier 1 â€” Router: <code>Llama-3.1-8B-Instruct</code> â†’ port 8001"},
      {b:"ğŸ·ï¸",t:"Tier 2 â€” Reasoning: <code>Qwen3-32B</code> (veya Qwen3-30B-A3B MoE) â†’ port 8002"},
      {b:"ğŸ·ï¸",t:"Tier 3 â€” Coding: <code>Qwen3-Coder-30B-Instruct</code> â†’ port 8003"},
    ],cmd:`<span class="c"># ğŸš€ Router â€” hÄ±zlÄ±, hafif</span>
docker run -d --gpus all --shm-size=8g \\
  --name vllm-router \\
  -v ~/.cache/huggingface:/root/.cache/huggingface \\
  -p 8001:8000 \\
  nvcr.io/nvidia/vllm:26.01-py3 \\
  vllm serve meta-llama/Llama-3.1-8B-Instruct \\
    --enforce-eager \\
    --max-model-len 8192 \\
    --gpu-memory-utilization 0.08 \\
    --max-num-seqs 16

<span class="c"># ğŸ§  Reasoning â€” derin dÃ¼ÅŸÃ¼nme</span>
docker run -d --gpus all --shm-size=16g \\
  --name vllm-reasoning \\
  -v ~/.cache/huggingface:/root/.cache/huggingface \\
  -p 8002:8000 \\
  nvcr.io/nvidia/vllm:26.01-py3 \\
  vllm serve Qwen/Qwen3-32B \\
    --enforce-eager \\
    --max-model-len 32768 \\
    --gpu-memory-utilization 0.35 \\
    --max-num-seqs 4

<span class="c"># ğŸ’» Coder â€” kod Ã¼retimi</span>
docker run -d --gpus all --shm-size=16g \\
  --name vllm-coder \\
  -v ~/.cache/huggingface:/root/.cache/huggingface \\
  -p 8003:8000 \\
  nvcr.io/nvidia/vllm:26.01-py3 \\
  vllm serve Qwen/Qwen3-Coder-30B-Instruct \\
    --enforce-eager \\
    --max-model-len 32768 \\
    --gpu-memory-utilization 0.35 \\
    --max-num-seqs 4`,
    extra:[
      {b:"ğŸ’¾",t:"UMA: ÃœÃ§ model aynÄ± 119 GB bellek havuzunu paylaÅŸÄ±r â€” <code>gpu-memory-utilization</code> toplam oranÄ±"},
      {b:"ğŸ“",t:"Tahsis: ~0.08 + 0.35 + 0.35 = 0.78 â†’ OS + KV cache iÃ§in ~25 GB kalÄ±r"},
      {b:"ğŸ§¹",t:"OOM durumunda: <code>sync && echo 3 > /proc/sys/vm/drop_caches</code>"},
      {b:"ğŸ“Š",t:"Bellek takibi: <code>free -h</code> kullan (nvidia-smi UMA'da N/A gÃ¶sterir)"},
    ]},
    {name:"Model YÃ¶nlendirme MantÄ±ÄŸÄ±",emoji:"ğŸ”€",tag:"ORKESTRASYON",tc:"#b08cff",items:[
      {b:"ğŸ”€",t:"GÃ¶rev tÃ¼rÃ¼ne gÃ¶re istekleri doÄŸru modele yÃ¶nlendiren HTTP proxy veya OpenClaw skill:"},
      {b:"âš¡",t:"Niyet tespiti / sÄ±nÄ±flandÄ±rma â†’ Llama 8B (port 8001) â€” hÄ±zlÄ±, <100ms hedef"},
      {b:"ğŸ§ ",t:"Planlama, mimari kararlar, karmaÅŸÄ±k reasoning â†’ Qwen 32B (port 8002)"},
      {b:"ğŸ’»",t:"Kod Ã¼retimi, refactoring, bug fix â†’ Qwen Coder 30B (port 8003)"},
      {b:"ğŸ”„",t:"YÃ¶nlendirme kararÄ± Llama 8B tarafÄ±ndan verilir: prompt â†’ sÄ±nÄ±fla â†’ ilgili modele ilet"},
      {b:"ğŸ›Ÿ",t:"Fallback: model hata/timeout â†’ aynÄ± modelde 1 retry â†’ Claude API'ye yÃ¼kselt"},
    ]},
    {name:"OpenClaw Entegrasyonu",emoji:"ğŸ”Œ",tag:"ORKESTRASYON",tc:"#b08cff",items:[
      {b:"ğŸ“¦",t:"Kurulum: <code>npm install -g openclaw && openclaw onboard</code>"},
      {b:"âš™ï¸",t:"Lokal vLLM endpoint'lerini OpenAI-uyumlu API olarak yapÄ±landÄ±r:"},
    ],cmd:`<span class="c">// ~/.openclaw/config.json</span>
{
  "models": {
    "providers": {
      "vllm-router": {
        "baseUrl": "http://localhost:8001/v1",
        "api": "openai-responses",
        "models": [{"id": "meta-llama/Llama-3.1-8B-Instruct",
                     "name": "Router", "contextWindow": 8192}]
      },
      "vllm-reasoning": {
        "baseUrl": "http://localhost:8002/v1",
        "api": "openai-responses",
        "models": [{"id": "Qwen/Qwen3-32B",
                     "name": "Reasoning", "contextWindow": 32768}]
      },
      "vllm-coder": {
        "baseUrl": "http://localhost:8003/v1",
        "api": "openai-responses",
        "models": [{"id": "Qwen/Qwen3-Coder-30B-Instruct",
                     "name": "Coder", "contextWindow": 32768}]
      },
      "anthropic": {
        "apiKey": "$ANTHROPIC_API_KEY",
        "models": [{"id": "claude-sonnet-4-5-20250929"}]
      }
    }
  }
}`,
    extra:[
      {b:"ğŸ¤–",t:"OpenClaw: konuÅŸma baÄŸlamÄ±, tool Ã§alÄ±ÅŸtÄ±rma ve agent yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼ yÃ¶netir"},
      {b:"ğŸ’¬",t:"Telegram/Slack kanal baÄŸlantÄ±sÄ± (Faz 1'de opsiyonel)"},
    ]},
    {name:"Coding Agent Ana DÃ¶ngÃ¼sÃ¼",emoji:"ğŸ”„",tag:"AGENT",tc:"#4d94ff",items:[
      {b:"1ï¸âƒ£",t:"Agent kodlama gÃ¶revi alÄ±r (API veya mesajlaÅŸma kanalÄ±ndan)"},
      {b:"2ï¸âƒ£",t:"Router (Llama 8B) sÄ±nÄ±flar: Ã¼ret / refactor / debug / aÃ§Ä±kla"},
      {b:"3ï¸âƒ£",t:"Coder (Qwen Coder 30B) sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ gÃ¶reve gÃ¶re kod Ã¼retir"},
      {b:"4ï¸âƒ£",t:"Kod sandbox Ã§alÄ±ÅŸma dizinine yazÄ±lÄ±r"},
      {b:"5ï¸âƒ£",t:"Linter Ã§alÄ±ÅŸÄ±r: Python â†’ <code>ruff check</code>, JS/TS â†’ <code>eslint</code>"},
      {b:"6ï¸âƒ£",t:"Derleme/sÃ¶zdizimi kontrolÃ¼: <code>python -c 'compile(...)'</code> veya <code>tsc --noEmit</code>"},
      {b:"7ï¸âƒ£",t:"Lint/derleme baÅŸarÄ±sÄ±z â†’ kod + hatalar Coder'a geri gÃ¶nderilir (Faz 1'de max 2 deneme)"},
      {b:"8ï¸âƒ£",t:"Son kod + durum (baÅŸarÄ±lÄ±/baÅŸarÄ±sÄ±z/kÄ±smi) talep edene dÃ¶ndÃ¼rÃ¼lÃ¼r"},
    ]},
    {name:"Sandbox & GÃ¼venlik",emoji:"ğŸ”’",tag:"GÃœVENLÄ°K",tc:"#ff5c6e",items:[
      {b:"ğŸ³",t:"Kod Ã§alÄ±ÅŸtÄ±rma izole Docker container veya chroot'ta OLMALI â€” asla host Ã¼zerinde deÄŸil"},
      {b:"ğŸš«",t:"Sandbox'ta aÄŸ eriÅŸimi yok (<code>--network=none</code>)"},
      {b:"ğŸ“",t:"Salt-okunur dosya sistemi â€” sadece <code>/tmp/workspace</code> yazÄ±labilir"},
      {b:"â±ï¸",t:"Ã‡alÄ±ÅŸtÄ±rma zaman aÅŸÄ±mÄ±: Ã§alÄ±ÅŸtÄ±rma baÅŸÄ±na 30 saniye"},
      {b:"ğŸ”‘",t:"Host ortam deÄŸiÅŸkenleri, secret'lar, SSH anahtarlarÄ±na sandbox'tan eriÅŸim yok"},
      {b:"ğŸ›¡ï¸",t:"PII maskeleme: tÃ¼m prompt'lar modele gÃ¶nderilmeden e-posta, API key, ÅŸifre taranÄ±r"},
      {b:"ğŸ”",t:"Regex tarama: <code>/[A-Za-z0-9+\\/=]{32,}/</code> token'lar, <code>/sk-[a-zA-Z0-9]{20,}/</code> API key'ler"},
    ]},
    {name:"Kill Switch (Acil Durdurma)",emoji:"ğŸ›‘",tag:"GÃœVENLÄ°K",tc:"#ff5c6e",items:[
      {b:"ğŸ“±",t:"Telegram komutu: <code>/kill</code> â†’ tÃ¼m vLLM container'larÄ±nÄ± anÄ±nda durdurur"},
      {b:"ğŸ³",t:"Uygulama: <code>docker stop vllm-router vllm-reasoning vllm-coder</code>"},
      {b:"âš™ï¸",t:"Sistem seviyesi: <code>systemctl stop openclaw</code> â†’ agent runtime durur"},
      {b:"ğŸ‘ï¸",t:"Cron watchdog: GPU bellek >%95 2 dk boyunca â†’ vLLM container'larÄ±nÄ± oto-yeniden baÅŸlat"},
      {b:"ğŸ”Œ",t:"Spark yanÄ±tsÄ±z kalÄ±rsa (UMA OOM zombie): DGX Dashboard veya fiziksel gÃ¼Ã§ dÃ¶ngÃ¼sÃ¼"},
    ]},
    {name:"Temel Loglama",emoji:"ğŸ“",tag:"GÃ–ZLEM",tc:"#ffb84d",items:[
      {b:"ğŸ“Š",t:"Her istek loglanÄ±r: <code>timestamp | model | tokens_in | tokens_out | latency_ms | status</code>"},
      {b:"ğŸ’¾",t:"Append-only SQLite log: <code>/var/log/coding-agent/requests.db</code>"},
      {b:"ğŸ”„",t:"GÃ¼nlÃ¼k rotasyon, 30 gÃ¼n saklama"},
      {b:"ğŸ“‹",t:"YapÄ±landÄ±rÄ±lmÄ±ÅŸ JSON loglarÄ± (OpenClaw Ã§Ä±ktÄ±sÄ±)"},
    ]},
  ],
  gov:{
    "EÅŸzamanlÄ± Ä°stek":"Model baÅŸÄ±na 4",
    "Batch Boyutu":"Router: 16, Reasoning: 4, Coder: 4",
    "Max Token":"Router: 8K, Reasoning: 32K, Coder: 32K",
    "Yeniden Deneme":"Ä°stek baÅŸÄ±na 1, sonra hata veya Claude fallback",
    "Kill Switch":"Telegram /kill â†’ docker stop",
    "GPU SÃ¼re":"HenÃ¼z takip edilmiyor (Faz 3)",
    "PII Maskeleme":"TÃ¼m gelen prompt'larda regex tarama",
  },
  success:[
    "Agent kodlama gÃ¶revi alÄ±yor ve sÃ¶zdizimsel olarak geÃ§erli kod Ã¼retiyor",
    "Linter Ã§Ä±ktÄ±da geÃ§iyor (ruff/eslint)",
    "Derleme kontrolÃ¼ geÃ§iyor (sÃ¶zdizimi hatasÄ± yok)",
    "Lint/derleme baÅŸarÄ±sÄ±z olursa: agent 1 kez deniyor ve dÃ¼zeltilmiÅŸ kodu dÃ¶ndÃ¼rÃ¼yor",
    "ÃœÃ§ model OOM olmadan eÅŸzamanlÄ± sunuyor",
    "Kill switch tÃ¼m servisleri 5 saniye iÃ§inde durduruyor",
    "TÃ¼m istekler SQLite'a model, token, gecikme bilgisiyle loglanÄ±yor",
  ]
},
// â•â•â•â•â•â• FAZ 2 â•â•â•â•â•â•
{
  id:2, emoji:"ğŸ”„", title:"Oto-DÃ¼zeltme & Kalite KatmanÄ±", color:"#4d94ff",
  obj:"Coding agent'Ä± Ã¼retime hazÄ±r hale getir: oto-dÃ¼zeltme dÃ¶ngÃ¼sÃ¼, semantik Ã¶nbellek, prompt versiyonlama ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ gÃ¶zlemlenebilirlik. HÃ¢lÃ¢ tek dÃ¼ÄŸÃ¼m inference.",
  s1:["vLLM inference (Faz 1 ile aynÄ±)","GeliÅŸtirilmiÅŸ agent mantÄ±ÄŸÄ±","Semantik Ã¶nbellek"],
  s2:["HÃ¢lÃ¢ boÅŸta veya sadece geliÅŸtirme testi"],
  blocks:[
    {name:"Oto-DÃ¼zeltme DÃ¶ngÃ¼sÃ¼",emoji:"ğŸ”§",tag:"AGENT",tc:"#4d94ff",items:[
      {b:"ğŸ“‹",t:"Ãœret â†’ lint â†’ dÃ¼zelt dÃ¶ngÃ¼sÃ¼nÃ¼ yapÄ±landÄ±rÄ±lmÄ±ÅŸ pipeline'a yÃ¼kselt:"},
      {b:"1ï¸âƒ£",t:"Coder kod Ã¼retir"},
      {b:"2ï¸âƒ£",t:"Statik analiz: linter + tip kontrolÃ¼ (<code>ruff check + mypy</code>)"},
      {b:"3ï¸âƒ£",t:"Hata bulunursa â†’ hata baÄŸlamÄ± oluÅŸtur: orijinal prompt + Ã¼retilen kod + hata Ã§Ä±ktÄ±sÄ±"},
      {b:"4ï¸âƒ£",t:"Hata baÄŸlamÄ±nÄ± Coder'a geri gÃ¶nder: 'Bu hatalarÄ± dÃ¼zelt. Sadece dÃ¼zeltilmiÅŸ kodu dÃ¶ndÃ¼r.'"},
      {b:"5ï¸âƒ£",t:"DÃ¼zeltilmiÅŸ kodda statik analizi tekrar Ã§alÄ±ÅŸtÄ±r"},
      {b:"ğŸ”„",t:"Yeniden deneme politikasÄ±: max 2 dÃ¼zeltme denemesi. 2 baÅŸarÄ±sÄ±zlÄ±k â†’ kÄ±smi sonuÃ§ + hata raporu"},
      {b:"ğŸ“Š",t:"DÃ¼zeltme baÅŸarÄ± oranÄ± takibi: <code>denenen</code> vs <code>Ã§Ã¶zÃ¼len</code>"},
    ]},
    {name:"Ä°stek SÄ±nÄ±flandÄ±rmasÄ±",emoji:"ğŸ“Š",tag:"YÃ–NLENDÄ°RME",tc:"#b08cff",items:[
      {b:"ğŸ“‹",t:"Gelen istekleri iki moda sÄ±nÄ±fla:"},
      {b:"âš¡",t:"Ä°nteraktif: kullanÄ±cÄ± yanÄ±t bekliyor â†’ gecikme optimize, max 30sn timeout, kÃ¼Ã§Ã¼k context"},
      {b:"ğŸ“¦",t:"Batch: arka plan gÃ¶revi (Ã¶r: modÃ¼l refactor) â†’ verimlilik optimize, timeout yok, tam context"},
      {b:"ğŸ”€",t:"SÄ±nÄ±flandÄ±rma Router (Llama 8B) tarafÄ±ndan kaynak kanal ve prompt metadata'sÄ±na gÃ¶re yapÄ±lÄ±r"},
      {b:"ğŸ¥‡",t:"Ä°nteraktif istekler Ã¶ncelikli kuyruk pozisyonu alÄ±r; batch istekler GPU boÅŸken Ã§alÄ±ÅŸÄ±r"},
    ]},
    {name:"Semantik Ã–nbellek",emoji:"ğŸ’¾",tag:"PERFORMANS",tc:"#00d4ff",items:[
      {b:"ğŸ”‘",t:"TamamlanmÄ±ÅŸ kod Ã¼retim sonuÃ§larÄ± normalize prompt hash'ine gÃ¶re Ã¶nbelleÄŸe alÄ±nÄ±r"},
      {b:"ğŸ’¾",t:"Ã–nbellek backend: SQLite + FTS5 (bulanÄ±k eÅŸleÅŸme) veya Redis"},
      {b:"ğŸ“",t:"Ã–nbellek isabet kriteri: prompt embedding'de kosinÃ¼s benzerliÄŸi > 0.95"},
      {b:"â°",t:"TTL: Ä°nteraktif 24 saat, batch sonuÃ§larÄ± 7 gÃ¼n"},
      {b:"ğŸ§¹",t:"Ã–nbellek temizleme: Telegram <code>/cache-clear</code> komutu"},
      {b:"ğŸ“ˆ",t:"Beklenen fayda: tekrarlayan gÃ¶revlerde model Ã§aÄŸrÄ±larÄ±nda %30-50 azalma"},
    ]},
    {name:"Prompt KayÄ±t Defteri & Versiyonlama",emoji:"ğŸ“š",tag:"KALÄ°TE",tc:"#00e68a",items:[
      {b:"ğŸ“",t:"TÃ¼m sistem prompt'larÄ± versiyonlu dosyalarda: <code>/etc/coding-agent/prompts/v{N}/</code>"},
      {b:"ğŸ“„",t:"Dizin yapÄ±sÄ±: <code>router.txt</code>, <code>coder-generate.txt</code>, <code>coder-fix.txt</code>, <code>reasoning.txt</code>"},
      {b:"ğŸ”¢",t:"Ä°stek logunda versiyon takibi: <code>prompt_version</code> sÃ¼tunu"},
      {b:"ğŸš«",t:"Prompt deÄŸiÅŸiklikleri aÃ§Ä±k versiyon artÄ±ÅŸÄ± gerektirir â€” hot-patch yok"},
      {b:"ğŸ”¬",t:"A/B testi: iki prompt versiyonu eÅŸzamanlÄ± Ã§alÄ±ÅŸtÄ±r, dÃ¼zeltme oranlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r"},
      {b:"â†©ï¸",t:"Geri alma: <code>PROMPT_VERSION=N</code> env deÄŸiÅŸkeni ile Ã¶nceki versiyona dÃ¶n"},
    ]},
    {name:"GÃ¶zlemlenebilirlik â€” YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ä°zleme",emoji:"ğŸ“¡",tag:"GÃ–ZLEM",tc:"#ffb84d",items:[
      {b:"ğŸ”—",t:"Her istek benzersiz <code>trace_id</code> (UUID v4) alÄ±r â€” tÃ¼m adÄ±mlara yayÄ±lÄ±r"},
      {b:"ğŸ“Š",t:"AdÄ±m baÅŸÄ±na log: <code>trace_id | adÄ±m | model | tokens_in | tokens_out | latency_ms | durum | prompt_version</code>"},
      {b:"ğŸ”„",t:"Ã‡ok adÄ±mlÄ± izlemeler baÄŸlantÄ±lÄ±: Ã¼ret â†’ lint â†’ dÃ¼zelt â†’ yeniden-lint hepsi aynÄ± trace_id"},
      {b:"ğŸ“ˆ",t:"Saatlik toplu metrikler (cron):"},
      {b:"â†’",t:"Model baÅŸÄ±na p50/p95/p99 gecikme"},
      {b:"â†’",t:"Model baÅŸÄ±na token verimi (tok/s)"},
      {b:"â†’",t:"DÃ¼zeltme baÅŸarÄ± oranÄ± (dÃ¼zeltilen / denenen)"},
      {b:"â†’",t:"Ã–nbellek isabet oranÄ±"},
      {b:"â†’",t:"Kategori bazÄ±nda hata oranÄ± (OOM, timeout, model hatasÄ±, lint hatasÄ±)"},
      {b:"ğŸ–¥ï¸",t:"Dashboard: saatlik oluÅŸturulan statik HTML, port 9090'da sunulur"},
    ]},
  ],
  gov:{
    "EÅŸzamanlÄ± Ä°stek":"Ä°nteraktif: 2, Batch: 2 (toplam 4)",
    "Batch Boyutu":"Router: 16, Reasoning: 4, Coder: 4",
    "Max Token":"Ä°stek baÅŸÄ±na: 32K. Dakika baÅŸÄ±na: tÃ¼m modellerde 200K",
    "Yeniden Deneme":"GÃ¶rev baÅŸÄ±na 2 dÃ¼zeltme. Model hatasÄ±nda 1 retry.",
    "Kill Switch":"Telegram /kill + /pause (dÃ¼zgÃ¼n boÅŸaltma) + /resume",
    "GPU SÃ¼re":"HenÃ¼z takip edilmiyor",
    "PII Maskeleme":"Regex + anahtar kelime kara listesi tarama",
  },
  success:[
    "Oto-dÃ¼zeltme lint/derleme hatalarÄ±nÄ±n >%60'Ä±nÄ± 2 denemede Ã§Ã¶zÃ¼yor",
    "Semantik Ã¶nbellek tekrarlayan gÃ¶rev kalÄ±plarÄ±nda >%20 isabet oranÄ±",
    "TÃ¼m prompt'lar versiyon takipli; geri alma test edildi ve Ã§alÄ±ÅŸÄ±yor",
    "Her istek trace_id ile uÃ§tan uca izlenebilir",
    "Saatlik metrik dashboard'u gecikme, verimlilik, hata oranlarÄ±nÄ± gÃ¶steriyor",
    "Ä°nteraktif istekler tipik gÃ¶revlerde <30 saniyede tamamlanÄ±yor",
  ]
},
// â•â•â•â•â•â• FAZ 3 â•â•â•â•â•â•
{
  id:3, emoji:"âš¡", title:"Dual Spark KullanÄ±mÄ±", color:"#b08cff",
  obj:"Spark #2'yi governance, audit ve aÄŸÄ±r geliÅŸtirme modeli iÃ§in devreye al. Tensor parallelism iÃ§in aÄŸ doÄŸrulamasÄ±. HÄ±z sÄ±nÄ±rlama ve GPU sÃ¼re muhasebesi.",
  s1:["Ãœretim coding agent (deÄŸiÅŸmez)","vLLM router + reasoning + coder"],
  s2:["Policy engine (FastAPI)","Audit log (PostgreSQL + Grafana)","AÄŸÄ±r dev model (batch gÃ¶revler)"],
  blocks:[
    {name:"Spark #2 Rol AtamasÄ±",emoji:"ğŸ—ï¸",tag:"MÄ°MARÄ°",tc:"#b08cff",items:[
      {b:"ğŸ“‹",t:"Spark #2 Ã¼Ã§ servis Ã§alÄ±ÅŸtÄ±rÄ±r:"},
      {b:"ğŸ›¡ï¸",t:"1. Policy Engine: Her agent aksiyonunu kurallara gÃ¶re deÄŸerlendiren Python FastAPI servisi"},
      {b:"ğŸ“Š",t:"2. Audit & GÃ¶zlemlenebilirlik: KalÄ±cÄ± metrik ve iz depolama iÃ§in PostgreSQL + Grafana"},
      {b:"ğŸ’»",t:"3. AÄŸÄ±r Dev Model: Batch/arka plan gÃ¶revleri iÃ§in ayrÄ±lmÄ±ÅŸ Qwen Coder 30B instance"},
      {b:"ğŸ­",t:"Spark #1 Ã¼retim yÃ¼zlÃ¼ inference dÃ¼ÄŸÃ¼mÃ¼ olarak kalÄ±r â€” kurulumunda deÄŸiÅŸiklik yok"},
      {b:"ğŸ”—",t:"Ä°letiÅŸim: Spark #1 â†’ Spark #2 arasÄ± HTTP, 200GbE baÄŸlantÄ± Ã¼zerinden"},
    ]},
    {name:"Policy Engine (Spark #2)",emoji:"ğŸ›¡ï¸",tag:"GOVERNANCE",tc:"#ff5c6e",items:[
      {b:"ğŸŒ",t:"Port 8100'de FastAPI servisi â€” her agent aksiyonu Ã§alÄ±ÅŸtÄ±rÄ±lmadan Ã¶nce buradan geÃ§er"},
      {b:"ğŸ“‹",t:"Kural deÄŸerlendirme: JSON policy dosyasÄ± tier'larÄ± tanÄ±mlar:"},
      {b:"ğŸŸ¢",t:"Tier 1 (oto-onay): kod Ã¼retimi, lint, dosya okuma, arama"},
      {b:"ğŸŸ¡",t:"Tier 2 (insan onayÄ±): harici API'ye yazma, mesaj gÃ¶nderme, deploy"},
      {b:"ğŸ”´",t:"Tier 3 (engellendi): Ã¼retim verisi silme, sistem dosyasÄ± deÄŸiÅŸtirme, secret eriÅŸimi"},
      {b:"â±ï¸",t:"HÄ±z sÄ±nÄ±rlama: agent baÅŸÄ±na, model baÅŸÄ±na, saat baÅŸÄ±na token bÃ¼tÃ§eleri"},
      {b:"ğŸ”„",t:"DÃ¶ngÃ¼ tespiti: aynÄ± agent 5 dk'da >10 Ã¶zdeÅŸ istek â†’ devre kesici"},
      {b:"ğŸ“",t:"TÃ¼m policy kararlarÄ± audit trail'a loglanÄ±r"},
    ]},
    {name:"GPU SÃ¼re Muhasebesi",emoji:"â±ï¸",tag:"GOVERNANCE",tc:"#ff5c6e",items:[
      {b:"ğŸ“Š",t:"Ä°stek baÅŸÄ±na GPU sÃ¼resi takibi: <code>inference_start</code> â†’ <code>inference_end</code>"},
      {b:"ğŸ“‹",t:"Toplama: agent, model, saat, gÃ¼n bazÄ±nda"},
      {b:"â°",t:"GÃ¼nlÃ¼k bÃ¼tÃ§e: her agent'a gÃ¼nde max 4 saat GPU sÃ¼resi"},
      {b:"ğŸ“ˆ",t:"HaftalÄ±k rapor: agent baÅŸÄ±na, model baÅŸÄ±na toplam GPU saati"},
      {b:"âš ï¸",t:"UyarÄ±: agent gÃ¼nlÃ¼k bÃ¼tÃ§enin %80'ini aÅŸarsa â†’ Telegram bildirimi"},
      {b:"ğŸš«",t:"Engelleme: %100 aÅŸÄ±lÄ±rsa â†’ istekler ertesi gÃ¼ne kadar kuyruÄŸa alÄ±nÄ±r"},
    ]},
    {name:"HÄ±z SÄ±nÄ±rlama",emoji:"ğŸš¦",tag:"GOVERNANCE",tc:"#ff5c6e",items:[
      {b:"ğŸ“Š",t:"Model baÅŸÄ±na limitler (dakika baÅŸÄ±na istek):"},
      {b:"âš¡",t:"Router (Llama 8B): 60 RPM"},
      {b:"ğŸ§ ",t:"Reasoning (Qwen 32B): 10 RPM"},
      {b:"ğŸ’»",t:"Coder (Qwen Coder 30B): 10 RPM"},
      {b:"â˜ï¸",t:"Claude API fallback: 5 RPM, aylÄ±k $150 sabit tavan"},
      {b:"ğŸ”§",t:"Uygulama: Spark #2'de Redis sliding window counter"},
      {b:"ğŸš«",t:"AÅŸÄ±lÄ±rsa â†’ HTTP 429 â†’ agent bekler ve pencere sonrasÄ±nda yeniden dener"},
    ]},
    {name:"Tensor Parallel AÄŸ DoÄŸrulama (Opsiyonel)",emoji:"ğŸ§ª",tag:"DENEYSEL",tc:"#ffb84d",items:[
      {b:"âš ï¸",t:"Bu keÅŸif amaÃ§lÄ±dÄ±r â€” Ã¼retim coding agent iÃ§in ZORUNLU DEÄÄ°L"},
      {b:"ğŸ”—",t:"Ray cluster oluÅŸturmayÄ± test et: head Spark #1, worker Spark #2"},
      {b:"ğŸ“¡",t:"InfiniBand/200GbE baÄŸlantÄ± Ã¼zerinden NCCL iletiÅŸimini doÄŸrula"},
      {b:"ğŸ§ª",t:"Ã–nce kÃ¼Ã§Ã¼k modelle test: Llama 8B ile <code>--tensor-parallel-size 2</code>"},
      {b:"ğŸ“",t:"YÃ¼k Ã¶lÃ§Ã¼mÃ¼: aynÄ± model iÃ§in tek-dÃ¼ÄŸÃ¼m vs Ã§ift-dÃ¼ÄŸÃ¼m gecikme karÅŸÄ±laÅŸtÄ±r"},
      {b:"ğŸš«",t:"Gecikme cezasÄ± >%50 ise: tensor parallel'i Faz 4'e ertele"},
      {b:"âŒ",t:"Bu fazda 235B model Ã‡ALIÅTIRMA â€” yetersiz doÄŸrulama"},
    ]},
  ],
  gov:{
    "EÅŸzamanlÄ± Ä°stek":"Spark #1: 4 Ã¼retim. Spark #2: 2 batch.",
    "Batch Boyutu":"Faz 2 ile aynÄ±",
    "Max Token":"Saat baÅŸÄ±na: tÃ¼m modellerde 500K. GÃ¼n baÅŸÄ±na: 5M.",
    "Yeniden Deneme":"2 dÃ¼zeltme + 1 model retry. Claude: 1 retry.",
    "Kill Switch":"Telegram /kill (her iki Spark) + /kill-spark1 + /kill-spark2",
    "GPU SÃ¼re":"Agent baÅŸÄ±na takip. GÃ¼nlÃ¼k limit: 4 GPU-saat.",
    "PII Maskeleme":"Regex + anahtar kelime + policy engine doÄŸrulama",
  },
  success:[
    "Spark #2 policy engine, audit DB ve batch modeli eÅŸzamanlÄ± sunuyor",
    "TÃ¼m agent aksiyonlarÄ± Ã§alÄ±ÅŸtÄ±rÄ±lmadan Ã¶nce policy engine'den geÃ§iyor",
    "HÄ±z sÄ±nÄ±rlama uygulanÄ±yor: aÅŸÄ±lan istekler HTTP 429 alÄ±yor",
    "GPU sÃ¼re muhasebesi Telegram ile gÃ¼nlÃ¼k rapor Ã¼retiyor",
    "Spark'lar arasÄ± aÄŸ >20 GB/s'de doÄŸrulandÄ±",
    "Tensor parallel test edildiyse: gecikme yÃ¼kÃ¼ belgelendi",
  ]
},
// â•â•â•â•â•â• FAZ 4 â•â•â•â•â•â•
{
  id:4, emoji:"ğŸš€", title:"Performans Optimizasyonu", color:"#ffb84d",
  obj:"Opsiyonel ileri faz. Verimlilik artÄ±r, gecikme azalt, spekÃ¼latif Ã§Ã¶zÃ¼mleme ve batch zamanlama uygula. Ãœretim saÄŸlamlaÅŸtÄ±rma.",
  s1:["SpekÃ¼latif Ã§Ã¶zÃ¼mleme ile optimize edilmiÅŸ Ã¼retim inference","Batch zamanlama"],
  s2:["GeliÅŸmiÅŸ gÃ¶zlemlenebilirlik","Performans benchmark","Kapasite planlama"],
  blocks:[
    {name:"SpekÃ¼latif Ã‡Ã¶zÃ¼mleme (Speculative Decoding)",emoji:"âš¡",tag:"PERFORMANS",tc:"#ffb84d",items:[
      {b:"ğŸ”€",t:"Llama 8B'yi Qwen 32B iÃ§in taslak model olarak kullan"},
      {b:"ğŸ“‹",t:"Taslak model N aday token Ã¼retir â†’ hedef model tek forward pass'te doÄŸrular"},
      {b:"ğŸ“ˆ",t:"Beklenen fayda: Qwen 32B reasoning gÃ¶revlerinde 1.5-2x decode hÄ±zlanma"},
      {b:"âš™ï¸",t:"vLLM flag: <code>--speculative-model Llama-3.1-8B --num-speculative-tokens 5</code>"},
      {b:"âš ï¸",t:"vLLM 0.13+ gerektirir ve <code>--enforce-eager</code> ile Ã§alÄ±ÅŸmayabilir â€” dikkatli test et"},
      {b:"ğŸ”®",t:"sm_121 iÃ§in Triton/CUDA graph desteÄŸi olgunlaÅŸÄ±rsa: <code>--enforce-eager</code> kaldÄ±rÄ±labilir"},
    ]},
    {name:"Batch Zamanlama",emoji:"ğŸ“…",tag:"PERFORMANS",tc:"#ffb84d",items:[
      {b:"â±ï¸",t:"Zaman tabanlÄ± batch pencereleri: 100ms boyunca istekleri topla â†’ batch olarak gÃ¶nder"},
      {b:"ğŸŒ™",t:"Gece modu (00:00-06:00): tÃ¼m interaktif slotlar batch iÅŸleme iÃ§in serbest"},
      {b:"ğŸ”„",t:"Otomatik model deÄŸiÅŸtirme: gece modunda gerekirse daha bÃ¼yÃ¼k varyantlarÄ± yÃ¼kle"},
      {b:"ğŸ¥‡",t:"Ã–ncelik kuyruÄŸu: kritik/interaktif â†’ normal/batch â†’ arka plan/spekÃ¼latif"},
    ]},
    {name:"Bellek EÅŸik KontrolÃ¼ (Memory Watermark)",emoji:"ğŸ’¾",tag:"KARARLLIK",tc:"#ff5c6e",items:[
      {b:"ğŸŸ¢",t:"YeÅŸil: <%70 sistem belleÄŸi kullanÄ±mda â†’ normal Ã§alÄ±ÅŸma"},
      {b:"ğŸŸ¡",t:"SarÄ±: %70-85 â†’ max_num_seqs %50 azalt, Telegram uyarÄ±"},
      {b:"ğŸ”´",t:"KÄ±rmÄ±zÄ±: >%85 â†’ yeni istek kabul etme, mevcut olanlarÄ± boÅŸalt, uyar"},
      {b:"ğŸ’€",t:"Kritik: >%95 â†’ acil: <code>sync && echo 3 > /proc/sys/vm/drop_caches</code> + vLLM yeniden baÅŸlat"},
      {b:"ğŸ‘ï¸",t:"Watchdog cron: 30 saniyede bir kontrol, bellek durumunu logla, eÅŸiklerde oto-aksiyon"},
      {b:"ğŸ›Ÿ",t:"UMA zombie Ã¶nleme: SSH 5+ dk yanÄ±tsÄ±zsa â†’ Spark #2 uzaktan yeniden baÅŸlatma tetikler"},
    ]},
    {name:"GeliÅŸmiÅŸ GÃ¶zlemlenebilirlik",emoji:"ğŸ“¡",tag:"GÃ–ZLEM",tc:"#00d4ff",items:[
      {b:"ğŸ“Š",t:"Her vLLM instance'Ä±nda Prometheus metrik dÄ±ÅŸa aktarÄ±cÄ± (vLLM yerleÅŸik /metrics endpoint)"},
      {b:"ğŸ–¥ï¸",t:"Spark #2'de Grafana dashboard: gerÃ§ek zamanlÄ± GPU kullanÄ±m, kuyruk derinliÄŸi, token verimi"},
      {b:"ğŸš¨",t:"UyarÄ± kurallarÄ±: gecikme p99 >60s, hata oranÄ± >%5, bellek >%85"},
      {b:"ğŸ”",t:"Ä°stek tekrarlama: debug iÃ§in herhangi bir izlenen isteÄŸi aynÄ± prompt versiyonuyla yeniden Ã§alÄ±ÅŸtÄ±r"},
      {b:"ğŸ’°",t:"Maliyet muhasebesi: token kullanÄ±mÄ± Ã— bulut eÅŸdeÄŸeri fiyatlamayla istek baÅŸÄ±na tahmini $ deÄŸer"},
    ]},
    {name:"Verimlilik Benchmark",emoji:"ğŸ“",tag:"BENCHMARK",tc:"#00e68a",items:[
      {b:"ğŸ”§",t:"<code>llama-benchy</code> veya vLLM yerleÅŸik benchmark kullan"},
      {b:"ğŸ“Š",t:"Yakalanacak metrikler: token/saniye (prefill vs decode), ilk-token-sÃ¼resi, istek/dakika"},
      {b:"ğŸ“ˆ",t:"Her model iÃ§in Ã§eÅŸitli eÅŸzamanlÄ±lÄ±k seviyelerinde (1, 2, 4, 8) referans benchmark"},
      {b:"âš–ï¸",t:"Eager mod vs CUDA graph (mÃ¼mkÃ¼n olduÄŸunda) performans farkÄ±nÄ± karÅŸÄ±laÅŸtÄ±r"},
      {b:"ğŸ“‹",t:"SonuÃ§larÄ± kapasite planlama referansÄ± olarak belgele"},
    ]},
  ],
  gov:{
    "EÅŸzamanlÄ± Ä°stek":"Dinamik: bellek eÅŸiÄŸine gÃ¶re (4 yeÅŸil, 2 sarÄ±, 0 kÄ±rmÄ±zÄ±)",
    "Batch Boyutu":"Dinamik: bellek durumuna gÃ¶re ayarlanÄ±r",
    "Max Token":"Faz 3 ile aynÄ±",
    "Yeniden Deneme":"Faz 3 ile aynÄ±",
    "Kill Switch":"GeliÅŸmiÅŸ: bellek watchdog oto-kill + Spark'lar arasÄ± uzaktan kill",
    "GPU SÃ¼re":"Maliyet tahmini ile tam muhasebe",
    "PII Maskeleme":"Faz 3 ile aynÄ±",
  },
  success:[
    "SpekÃ¼latif Ã§Ã¶zÃ¼mleme reasoning gÃ¶revlerinde Ã¶lÃ§Ã¼lebilir hÄ±zlanma saÄŸlÄ±yor (>1.3x)",
    "Bellek eÅŸik sistemi 7 gÃ¼nlÃ¼k test sÃ¼resince OOM Ã§Ã¶kmelerini Ã¶nlÃ¼yor",
    "Grafana dashboard tÃ¼m modeller iÃ§in gerÃ§ek zamanlÄ± metrikleri gÃ¶steriyor",
    "Benchmark sonuÃ§larÄ± kapasite planlama iÃ§in belgelendi",
    "Gece modu batch iÅŸleme kuyruÄŸa alÄ±nmÄ±ÅŸ gÃ¶revleri 06:00'ya kadar tamamlÄ±yor",
  ]
}
];

// â•â•â•â•â•â• MODEL VERÄ°SÄ° â•â•â•â•â•â•
const modelTiers = [
  {tier:"Tier 1",emoji:"âš¡",role:"Router / SÄ±nÄ±flandÄ±rÄ±cÄ±",model:"Llama-3.1-8B-Instruct",quant:"FP16 veya NVFP4",ram:"~8-10 GB (FP16) / ~5 GB (FP4)",speed:"~50-90 tok/s",ctx:"8K (kasÄ±tlÄ± sÄ±nÄ±rlÄ±)",port:"8001",use:"GÃ¶rev sÄ±nÄ±flandÄ±rma, niyet tespiti, yÃ¶nlendirme kararlarÄ±. HÄ±zlÄ± yanÄ±t gerekli.",color:"#00d4ff"},
  {tier:"Tier 2",emoji:"ğŸ§ ",role:"Reasoning",model:"Qwen3-32B (veya 30B-A3B MoE)",quant:"FP16 / BF16",ram:"~35-40 GB (FP16) / ~20 GB (MoE aktif)",speed:"~15-25 tok/s",ctx:"32K",port:"8002",use:"Mimari planlama, karmaÅŸÄ±k debug, Ã§ok adÄ±mlÄ± reasoning. HÄ±zdan Ã§ok kalite.",color:"#00e68a"},
  {tier:"Tier 3",emoji:"ğŸ’»",role:"Coding",model:"Qwen3-Coder-30B-Instruct",quant:"FP16 / BF16",ram:"~35 GB",speed:"~15-25 tok/s",ctx:"32K",port:"8003",use:"Kod Ã¼retimi, refactoring, bug fix, test yazma. Birincil kodlama modeli.",color:"#4d94ff"},
  {tier:"Tier X",emoji:"â˜ï¸",role:"Fallback",model:"Claude Sonnet 4 (API)",quant:"Cloud",ram:"N/A",speed:"~80+ tok/s",ctx:"200K",port:"API",use:"Lokal model baÅŸarÄ±sÄ±z olduÄŸunda veya timeout aldÄ±ÄŸÄ±nda fallback. AylÄ±k $150 bÃ¼tÃ§e sÄ±nÄ±rÄ±.",color:"#555568"},
];

const ramItems = [
  {l:"Llama 3.1 8B",gb:10,c:"#00d4ff"},
  {l:"Qwen3-32B",gb:38,c:"#00e68a"},
  {l:"Qwen Coder 30B",gb:35,c:"#4d94ff"},
  {l:"KV Cache (3 model)",gb:20,c:"#ffb84d"},
  {l:"OS + Docker + Agent",gb:8,c:"#555568"},
  {l:"BoÅŸ Alan (kritik)",gb:8,c:"#2a2a3a"},
];

// â•â•â•â•â•â• STATE â•â•â•â•â•â•
let tab = "p0";
let collapsed = new Set();

function setTab(t){ tab=t; R(); }
function togBlk(k){ collapsed.has(k)?collapsed.delete(k):collapsed.add(k); R(); }

// â•â•â•â•â•â• RENDER â•â•â•â•â•â•
function R(){
  const app = document.getElementById('app');
  app.innerHTML = `
    <div class="hero">
      <div class="chip"><span class="pulse"></span>MÃ¼hendislik PlanÄ± v2.0</div>
      <h1>Dual DGX Spark â€” Coding Agent</h1>
      <p>vLLM inference runtime â€¢ OpenClaw orkestrasyon â€¢ Deterministik governance â€¢ Oto-dÃ¼zeltme derleme dÃ¶ngÃ¼sÃ¼</p>
      <div class="hero-tags">
        <span class="hero-tag">ğŸ“ 2Ã— DGX Spark (128 GB)</span>
        <span class="hero-tag">ğŸ”§ vLLM + NGC Container</span>
        <span class="hero-tag">ğŸ—ï¸ ARM64 / Blackwell SM121</span>
        <span class="hero-tag">âš ï¸ enforce-eager zorunlu</span>
      </div>
    </div>

    <div class="specs">
      ${[
        {e:"ğŸ§ ",v:"128 GB",l:"Spark BaÅŸÄ±na RAM"},
        {e:"ğŸ’¾",v:"119 GB",l:"KullanÄ±labilir (UMA)"},
        {e:"âš¡",v:"SM121",l:"Compute Cap."},
        {e:"ğŸ”—",v:"200 GbE",l:"Spark BaÄŸlantÄ±"},
        {e:"ğŸ”§",v:"CUDA 13",l:"Toolkit"},
        {e:"ğŸ—ï¸",v:"ARM64",l:"Mimari"},
      ].map(s=>`<div class="spec"><div class="emoji">${s.e}</div><div class="val">${s.v}</div><div class="lbl">${s.l}</div></div>`).join('')}
    </div>

    <div class="nav">
      ${phases.map(p=>`<button class="${tab==='p'+p.id?'on':''}" onclick="setTab('p${p.id}')">${p.emoji} Faz ${p.id}</button>`).join('')}
      <button class="${tab==='models'?'on':''}" onclick="setTab('models')">ğŸ¤– Modeller</button>
    </div>

    ${tab==='models' ? renderModels() : ''}
    ${phases.map(p => tab==='p'+p.id ? renderPhase(p) : '').join('')}
  `;
}

function renderPhase(p){
  return `<div class="sec vis"><div class="ph">
    <div class="ph-hdr">
      <div class="ph-icon" style="background:${p.color}15;font-size:24px">${p.emoji}</div>
      <div class="ph-info"><h2 style="color:${p.color}">Faz ${p.id} â€” ${p.title}</h2><p>${p.obj}</p></div>
    </div>

    <div class="spark-grid">
      <div class="spark-card" style="border-color:${p.color}35">
        <h4>ğŸŸ¢ Spark #1<span class="badge">128 GB</span></h4>
        ${p.s1.map(s=>`<div class="si"><span class="arr" style="color:${p.color}">â†’</span>${s}</div>`).join('')}
      </div>
      <div class="spark-card" style="border-color:var(--brd)">
        <h4>ğŸ”µ Spark #2<span class="badge">128 GB</span></h4>
        ${p.s2.map(s=>`<div class="si"><span class="arr" style="color:var(--t3)">â†’</span>${s}</div>`).join('')}
      </div>
    </div>

    ${p.blocks.map((b,i)=>{
      const k=`p${p.id}-${i}`;
      const col = collapsed.has(k);
      return `<div class="blk">
        <div class="blk-h" onclick="togBlk('${k}')">
          <span class="left"><span>${b.emoji}</span><span>${b.name}</span></span>
          <span style="display:flex;align-items:center;gap:8px">
            <span class="tag" style="background:${b.tc}12;color:${b.tc};border:1px solid ${b.tc}30">${b.tag}</span>
            <span class="chevron">${col?'â–¸':'â–¾'}</span>
          </span>
        </div>
        <div class="blk-b ${col?'hide':''}">
          ${b.items.map(it=>`<div class="it"><span class="bullet">${it.b}</span><span>${it.t}</span></div>`).join('')}
          ${b.cmd?`<div class="cmd"><code>${b.cmd}</code></div>`:''}
          ${(b.extra||[]).map(it=>`<div class="it"><span class="bullet">${it.b}</span><span>${it.t}</span></div>`).join('')}
        </div>
      </div>`;
    }).join('')}

    <div class="blk">
      <div class="blk-h" onclick="togBlk('gov-${p.id}')">
        <span class="left"><span>ğŸ›¡ï¸</span><span>Governance Parametreleri</span></span>
        <span style="display:flex;align-items:center;gap:8px">
          <span class="tag" style="background:#ff5c6e12;color:#ff5c6e;border:1px solid #ff5c6e30">GOVERNANCE</span>
          <span class="chevron">${collapsed.has('gov-'+p.id)?'â–¸':'â–¾'}</span>
        </span>
      </div>
      <div class="blk-b ${collapsed.has('gov-'+p.id)?'hide':''}">
        <div class="gov">
          ${Object.entries(p.gov).map(([k,v])=>`
            <div class="gov-cell"><div class="gl">${k}</div><div class="gv">${v}</div></div>
          `).join('')}
        </div>
      </div>
    </div>

    <div class="success">
      <h4>âœ… BaÅŸarÄ± Kriterleri â€” Faz ${p.id} Tamamlanma KoÅŸullarÄ±:</h4>
      ${p.success.map(s=>`<div class="it"><span class="bullet" style="color:var(--grn)">âœ“</span><span>${s}</span></div>`).join('')}
    </div>
  </div></div>`;
}

function renderModels(){
  const total = ramItems.reduce((a,r)=>a+r.gb,0);
  const ok = total <= 119;
  return `<div class="sec vis"><div class="ph">
    <div class="ph-hdr">
      <div class="ph-icon" style="background:#00e68a15;font-size:24px">ğŸ¤–</div>
      <div class="ph-info"><h2>Model Stratejisi</h2><p>ÃœÃ§ katmanlÄ± lokal inference + bulut fallback. TÃ¼mÃ¼ vLLM ile sunulur (OpenAI-uyumlu API).</p></div>
    </div>

    <div class="warn">
      <h4>âš ï¸ DGX Spark â€” vLLM KÄ±sÄ±tlamalarÄ±</h4>
      <p>ğŸ”§ <strong>enforce-eager zorunlu:</strong> Triton/CUDA graph derleme sm_121 (Blackwell) iÃ§in henÃ¼z tam destek vermiyor. TÃ¼m vLLM instance'larÄ± <code>--enforce-eager</code> ile Ã§alÄ±ÅŸmalÄ±. Bu, optimize moda gÃ¶re ~%20-30 daha yavaÅŸ inference demek.</p>
      <p style="margin-top:6px">ğŸ’¾ <strong>UMA bellek:</strong> GPU ve CPU aynÄ± 128 GB LPDDR5x havuzunu paylaÅŸÄ±r. <code>nvidia-smi</code> Memory: N/A gÃ¶sterir. <code>free -h</code> ile takip et. OOM temiz CUDA hatasÄ± deÄŸil, sistem dondurmasÄ± (zombie) olarak ortaya Ã§Ä±kar.</p>
      <p style="margin-top:6px">ğŸ“¦ <strong>NGC container Ã¶nerilir:</strong> <code>nvcr.io/nvidia/vllm:26.01-py3</code> â€” ARM64 + CUDA 13 yamalarÄ± ile NVIDIA onaylÄ± build.</p>
    </div>

    <table class="tbl">
      <thead><tr><th></th><th>Tier</th><th>Rol</th><th>Model</th><th>RAM</th><th>HÄ±z</th><th>Context</th><th>Port</th></tr></thead>
      <tbody>
        ${modelTiers.map(m=>`<tr>
          <td>${m.emoji}</td>
          <td><strong>${m.tier}</strong></td>
          <td>${m.role}</td>
          <td style="font-family:'JetBrains Mono',monospace;font-size:11px;color:${m.color}">${m.model}</td>
          <td>${m.ram}</td>
          <td>${m.speed}</td>
          <td>${m.ctx}</td>
          <td style="font-family:'JetBrains Mono',monospace">${m.port}</td>
        </tr>`).join('')}
      </tbody>
    </table>

    ${modelTiers.map((m,i)=>`
      <div class="blk">
        <div class="blk-h" onclick="togBlk('m-${i}')">
          <span class="left"><span>${m.emoji}</span><span>${m.tier}: ${m.model}</span></span>
          <span style="display:flex;align-items:center;gap:8px">
            <span class="tag" style="background:${m.color}12;color:${m.color};border:1px solid ${m.color}30">${m.role}</span>
            <span class="chevron">${collapsed.has('m-'+i)?'â–¸':'â–¾'}</span>
          </span>
        </div>
        <div class="blk-b ${collapsed.has('m-'+i)?'hide':''}">
          <div class="it"><span class="bullet">ğŸ“¦</span><span><strong>Quantization:</strong> ${m.quant}</span></div>
          <div class="it"><span class="bullet">ğŸ’¾</span><span><strong>RAM:</strong> ${m.ram}</span></div>
          <div class="it"><span class="bullet">âš¡</span><span><strong>Beklenen hÄ±z:</strong> ${m.speed} (enforce-eager ile)</span></div>
          <div class="it"><span class="bullet">ğŸ“</span><span><strong>Context penceresi:</strong> ${m.ctx}</span></div>
          <div class="it"><span class="bullet">ğŸ¯</span><span><strong>KullanÄ±m:</strong> ${m.use}</span></div>
        </div>
      </div>
    `).join('')}

    <div class="blk" style="margin-top:14px">
      <div class="blk-h" onclick="togBlk('ram')">
        <span class="left"><span>ğŸ’¾</span><span>Spark #1 â€” Bellek BÃ¼tÃ§esi (119 GB kullanÄ±labilir)</span></span>
        <span style="display:flex;align-items:center;gap:8px">
          <span class="tag" style="background:#ffb84d12;color:#ffb84d;border:1px solid #ffb84d30">RAM</span>
          <span class="chevron">${collapsed.has('ram')?'â–¸':'â–¾'}</span>
        </span>
      </div>
      <div class="blk-b ${collapsed.has('ram')?'hide':''}">
        ${ramItems.map(r=>`
          <div class="ram-row">
            <div class="ram-lbl">${r.l}</div>
            <div class="ram-bg"><div class="ram-fill" style="width:${Math.max(r.gb/119*100,4)}%;background:${r.c}"><span>${r.gb} GB</span></div></div>
          </div>
        `).join('')}
        <div style="margin-top:10px;font-size:12px;color:var(--t3)">
          Toplam: ${total} GB / 119 GB.
          ${ok ? '<span style="color:#00e68a"> âœ… BÃ¼tÃ§e dahilinde.</span>' : '<span style="color:#ff5c6e"> âš ï¸ BÃ¼tÃ§e aÅŸÄ±mÄ± â€” context pencerelerini kÃ¼Ã§Ã¼lt veya quantize modeller kullan.</span>'}
        </div>

        <div class="danger" style="margin-top:12px">
          <h4>âš ï¸ UMA Bellek UyarÄ±sÄ±</h4>
          <div class="it"><span class="bullet">ğŸ’¾</span><span>DGX Spark Unified Memory Architecture kullanÄ±r â€” GPU ve CPU aynÄ± fiziksel RAM'i paylaÅŸÄ±r</span></div>
          <div class="it"><span class="bullet">ğŸ’€</span><span>OOM temiz CUDA hatasÄ± ÃœRETMEZ â€” sistem zombie olur (SSH donar, fiziksel yeniden baÅŸlatma gerekir)</span></div>
          <div class="it"><span class="bullet">ğŸ›Ÿ</span><span>OS, Docker ve agent sÃ¼reÃ§leri iÃ§in her zaman â‰¥8 GB boÅŸ alan bÄ±rak</span></div>
          <div class="it"><span class="bullet">ğŸ“Š</span><span><code>free -h</code> ile takip et â€” <code>nvidia-smi</code> DEÄÄ°L (UMA'da N/A gÃ¶sterir)</span></div>
          <div class="it"><span class="bullet">ğŸ”§</span><span>Her iki 30B+ model FP16 ise: sadece modeller iÃ§in ~75 GB â€” Ã§alÄ±ÅŸÄ±r ama sÄ±kÄ±</span></div>
          <div class="it"><span class="bullet">ğŸ“‰</span><span>Bellek baskÄ±sÄ± gÃ¶zlemlenirse Q4/Q8 quantization dÃ¼ÅŸÃ¼n</span></div>
        </div>
      </div>
    </div>

    <div class="blk" style="margin-top:8px">
      <div class="blk-h" onclick="togBlk('load-order')">
        <span class="left"><span>ğŸ“‹</span><span>Model YÃ¼kleme SÄ±rasÄ±</span></span>
        <span class="tag" style="background:#4d94ff12;color:#4d94ff;border:1px solid #4d94ff30">PROSEDÃœR</span>
      </div>
      <div class="blk-b ${collapsed.has('load-order')?'hide':''}">
        <div class="it"><span class="bullet">1ï¸âƒ£</span><span>Ã–nce Router'Ä± (Llama 8B) baÅŸlat â€” en kÃ¼Ã§Ã¼k ayak izi, saÄŸlÄ±k kontrolleri iÃ§in gerekli</span></div>
        <div class="it"><span class="bullet">2ï¸âƒ£</span><span>Coder'Ä± (Qwen Coder 30B) baÅŸlat â€” birincil iÅŸ yÃ¼kÃ¼ modeli</span></div>
        <div class="it"><span class="bullet">3ï¸âƒ£</span><span>Reasoning'i (Qwen 32B) en son yÃ¼kle â€” daha az sÄ±klÄ±kla kullanÄ±lÄ±r</span></div>
        <div class="it"><span class="bullet">4ï¸âƒ£</span><span>ÃœÃ§Ã¼ de Ã§alÄ±ÅŸÄ±rken: <code>free -h</code> ile belleÄŸi doÄŸrula</span></div>
        <div class="it"><span class="bullet">5ï¸âƒ£</span><span>KullanÄ±labilir bellek <8 GB ise: <code>--max-model-len</code> veya <code>--gpu-memory-utilization</code> azalt</span></div>
        <div class="it"><span class="bullet">6ï¸âƒ£</span><span>Ãœretim trafiÄŸi kabul etmeden Ã¶nce her modelde inference testi Ã§alÄ±ÅŸtÄ±r</span></div>
      </div>
    </div>

  </div></div>`;
}

R();
</script>
</body>
</html>
